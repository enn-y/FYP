{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**FYP - Lua Chong En [20417309]**\n",
        "\n",
        "**SpamLyte: Lightweight Stacking Approach for Spam Detection**\n",
        "\n",
        "**Jupyter Notebook Code - Google Colab**\n",
        "\n",
        "*Pre-requisites*\n",
        "- Have all dataset files (4,000 to 16,000) - Use file path: n.xlsx\n",
        "- Have the feature extracted datasets (TF-IDF) - Use file path: tfidf_file1000-n.xlsx\n",
        "- Ensure all files are uploaded onto Google Colab prior to executing any cells\n",
        "- Download all data from the kaggle link below\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "This notebook is split into 12 sections - Use the table to contents to navigate to each section\n",
        "\n",
        "\n",
        "1. Forcefully install nltk version 3.8.1 due to ongoing dependency problem\n",
        "2. Data Pre-processing\n",
        "3. Support Vector Machine (Classifier)\n",
        "4. Random Forest (Classifier)\n",
        "5. Multinomial Naive Bayes (Classifier)\n",
        "6. K-Nearest-Neighbors (Classifier)\n",
        "7. Logistic Regression (Meta-model)\n",
        "8. Stacking (MNB, SVM, DT) + (LR) - Combination 1\n",
        "9. Stacking (MNB, KNN, DT) + (LR) - Combination 2\n",
        "10. Stacking (MNB, Perceptron, DT) + (LR) - Combination 3\n",
        "11. Stacking (MNB, LightGBM, RF) + (LR) - Combination 4\n",
        "12. Final Stacking Technique (MNB, KNN, RF) + (LR) - Combination 5\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Datasets used in this research can be found here (Kaggle): https://kaggle.com/datasets/5cc5fc5934ccd6336b7ef938834d5038ad795ad66927783caceea6ad529cddad\n",
        "\n",
        "'\n",
        "\n",
        "--- Last modified on 30/04/2025 by Lua Chong En ---\n",
        "\n",
        "'"
      ],
      "metadata": {
        "id": "L1k1eG0yThJK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Forcefully install nltk version 3.8.1 due to ongoing dependency problem"
      ],
      "metadata": {
        "id": "cWfS9Vh_UAb3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbv4H5rhC6dV",
        "outputId": "ac242610-b798-46e7-b4ac-0afc51f7480d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nltk==3.8.1\n",
            "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (4.67.1)\n",
            "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nltk\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.9.1\n",
            "    Uninstalling nltk-3.9.1:\n",
            "      Successfully uninstalled nltk-3.9.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "textblob 0.19.0 requires nltk>=3.9, but you have nltk 3.8.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nltk-3.8.1\n",
            "3.8.1\n"
          ]
        }
      ],
      "source": [
        "#!pip uninstall nltk -y\n",
        "\n",
        "!pip install nltk==3.8.1\n",
        "\n",
        "import nltk\n",
        "print(nltk.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Data Pre-processing and Feature Extraction"
      ],
      "metadata": {
        "id": "MC8ahda3UKLi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhnVsq7YDVd6",
        "outputId": "052a328b-05ae-4fe6-86f5-a0997003c9f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of features detected in the data: 16300\n",
            "Elapsed time: 126.64 seconds\n",
            "Most important words for determining spam:\n",
            "received    0.121925\n",
            "aug         0.109860\n",
            "sep         0.098344\n",
            "id          0.090963\n",
            "wed         0.065102\n",
            "oct         0.063014\n",
            "mon         0.062805\n",
            "fri         0.051524\n",
            "postfix     0.039254\n",
            "jalapeno    0.035262\n",
            "dtype: float64\n",
            "\n",
            "Least important words for determining spam:\n",
            "generate     0.001210\n",
            "targeted     0.001209\n",
            "po           0.001194\n",
            "held         0.001158\n",
            "echo         0.001113\n",
            "broadcast    0.001108\n",
            "inquiry      0.001092\n",
            "ordering     0.000866\n",
            "judgment     0.000695\n",
            "cm           0.000427\n",
            "dtype: float64\n",
            "\n",
            "\n",
            "Finished\n"
          ]
        }
      ],
      "source": [
        "#DATA PRE-PROCESSING AND FEATURE EXTRACTION (TF-IDF)\n",
        "\n",
        "# Import necessary libraries\n",
        "import time\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_excel('/content/4000.xlsx', usecols=['text', 'target'])\n",
        "\n",
        "# Preprocess text - lowercase and remove non-alphabetic characters\n",
        "def preprocess_text(text):\n",
        "    if isinstance(text, str):\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    else:\n",
        "        text = ''\n",
        "    return text\n",
        "\n",
        "# Check if the word is valid using WordNet\n",
        "def is_valid_word(word):\n",
        "    return bool(wordnet.synsets(word))\n",
        "\n",
        "# Apply Tokenization, Lemmatization, Stop Word Removal\n",
        "def tokenize_lemmatize_stopword_blacklist(text):\n",
        "    if isinstance(text, str):  # Check if the text is a string\n",
        "        tokens = word_tokenize(text)\n",
        "\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "\n",
        "        lemmatized_tokens = [\n",
        "            lemmatizer.lemmatize(word)\n",
        "            for word in tokens\n",
        "            if word not in stop_words\n",
        "            and is_valid_word(word)\n",
        "        ]\n",
        "\n",
        "        return ' '.join(lemmatized_tokens)\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "df['text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "df['text'] = df['text'].apply(tokenize_lemmatize_stopword_blacklist)\n",
        "\n",
        "df['target'] = pd.to_numeric(df['target'], errors='coerce')\n",
        "\n",
        "# Apply TF-IDF Vectorization without limiting features - Feature Extraction\n",
        "full_vectorizer = TfidfVectorizer()\n",
        "full_tfidf_matrix = full_vectorizer.fit_transform(df['text'])\n",
        "\n",
        "# Find the total number of unique features (terms) in the data\n",
        "total_detected_features = len(full_vectorizer.get_feature_names_out())\n",
        "print(f\"Total number of features detected in the data: {total_detected_features}\")\n",
        "\n",
        "# Apply TF-IDF Vectorization with 1000 features\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "tfidf_matrix = vectorizer.fit_transform(df['text'])\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# Concatenate the original DataFrame with the TF-IDF DataFrame\n",
        "final_df = pd.concat([df.reset_index(drop=True), tfidf_df.reset_index(drop=True)], axis=1)\n",
        "\n",
        "# Save the final DataFrame to a new Excel file\n",
        "final_df.to_excel('/content/tfidf_file1000.xlsx', index=False)\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
        "\n",
        "# Retrieve the feature names\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Create a DataFrame to view the TF-IDF scores for each word across all documents\n",
        "tfidf_matrix = vectorizer.transform(df['text']).toarray()\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix, columns=feature_names)\n",
        "\n",
        "# Calculate the average TF-IDF score for each word across all documents\n",
        "average_tfidf_scores = tfidf_df.mean().sort_values(ascending=False)\n",
        "\n",
        "# Most important words (highest TF-IDF scores)\n",
        "most_important_words = average_tfidf_scores.head(10)\n",
        "\n",
        "# Least important words (lowest TF-IDF scores)\n",
        "least_important_words = average_tfidf_scores.tail(10)\n",
        "\n",
        "# Display the results\n",
        "print(\"Most important words for determining spam:\")\n",
        "print(most_important_words)\n",
        "print(\"\\nLeast important words for determining spam:\")\n",
        "print(least_important_words)\n",
        "print(\"\\n\\nFinished\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Support Vector Machine"
      ],
      "metadata": {
        "id": "CzyffDYwUeS7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlfIn-OEtQyw",
        "outputId": "115b0192-0542-4f0f-9166-3c81cae03644"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed time: 211.43 seconds\n",
            "--- Support Vector Machine Model ---\n",
            "Accuracy: 95.27%\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0      0.954     0.949     0.951      1463\n",
            "           1      0.951     0.956     0.954      1537\n",
            "\n",
            "    accuracy                          0.953      3000\n",
            "   macro avg      0.953     0.953     0.953      3000\n",
            "weighted avg      0.953     0.953     0.953      3000\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[1388   75]\n",
            " [  67 1470]]\n",
            "\n",
            "Successfully detected spam (True Positives): 1470\n",
            "\n",
            "RAM Usage: 1.59 GB\n"
          ]
        }
      ],
      "source": [
        "# SUPPORT VECTOR MACHINE\n",
        "\n",
        "import pandas as pd\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import psutil  # Import the psutil library for memory usage\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Load feature extracted dataset - TF-IDF\n",
        "df = pd.read_excel('/content/tfidf_file1000-10.xlsx')\n",
        "\n",
        "# Separate features (X) and target labels (y)\n",
        "X = df.drop(columns=['text', 'target'], errors='ignore')\n",
        "y = df['target']\n",
        "\n",
        "# Split the data into training and testing sets (70-30 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Support Vector Machine classifier\n",
        "svm_model = SVC(kernel='linear')\n",
        "\n",
        "# Train the model\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred, digits=3)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "ram_usage = psutil.virtual_memory().used / (1024 ** 3)\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"--- Support Vector Machine Model ---\")\n",
        "print(f\"Accuracy: {accuracy:.2%}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_rep)\n",
        "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
        "print(\"\\nSuccessfully detected spam (True Positives):\", conf_matrix[1, 1])\n",
        "print(f\"\\nRAM Usage: {ram_usage:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Random Forest"
      ],
      "metadata": {
        "id": "jQL91hkJUlm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#RANDOM FOREST\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import psutil  # For monitoring memory usage\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Load feature extracted dataset - TF-IDF\n",
        "df = pd.read_excel('/content/tfidf_file1000-10.xlsx')\n",
        "\n",
        "# Separate features (X) and target labels (y)\n",
        "X = df.drop(columns=['text', 'target'], errors='ignore')\n",
        "y = df['target']\n",
        "\n",
        "# Split the data into training and testing sets (70-30 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Random Forest classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "\n",
        "# Train the model\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred, digits=3)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "ram_usage = psutil.virtual_memory().used / (1024 ** 3)  # RAM usage in GB\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"--- Random Forest Model ---\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"\\nClassification Report:\\n\", classification_rep)\n",
        "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
        "print(\"\\nSuccessfully detected spam (True Positives):\", conf_matrix[1, 1])\n",
        "print(f\"\\nRAM Usage: {ram_usage:.2f} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjfbN_JHilPi",
        "outputId": "5b4af094-5e65-4fb2-c69e-7341fe2fd6a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed time: 196.91 seconds\n",
            "--- Random Forest Model ---\n",
            "Accuracy: 0.9596666666666667\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0      0.958     0.959     0.959      1463\n",
            "           1      0.961     0.960     0.961      1537\n",
            "\n",
            "    accuracy                          0.960      3000\n",
            "   macro avg      0.960     0.960     0.960      3000\n",
            "weighted avg      0.960     0.960     0.960      3000\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[1403   60]\n",
            " [  61 1476]]\n",
            "\n",
            "Successfully detected spam (True Positives): 1476\n",
            "\n",
            "RAM Usage: 8.34 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "PkCbJjSDUutn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dymwz1FalSbm",
        "outputId": "d614153d-ad27-40a6-fbbb-b559a2c3156d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed time: 181.96 seconds\n",
            "--- Multinomial Naive Bayes Model ---\n",
            "Accuracy: 0.9293333333333333\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0      0.911     0.948     0.929      1463\n",
            "           1      0.949     0.912     0.930      1537\n",
            "\n",
            "    accuracy                          0.929      3000\n",
            "   macro avg      0.930     0.930     0.929      3000\n",
            "weighted avg      0.930     0.929     0.929      3000\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[1387   76]\n",
            " [ 136 1401]]\n",
            "\n",
            "Successfully detected spam (True Positives): 1401\n",
            "\n",
            "RAM Usage: 1.76 GB\n"
          ]
        }
      ],
      "source": [
        "# MULTINOMIAL NAIVE BAYES\n",
        "\n",
        "import pandas as pd\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import psutil  # Import the psutil library for memory usage\n",
        "\n",
        "start_time = time.time()\n",
        "# Load feature extracted dataset - TF-IDF\n",
        "df = pd.read_excel('/content/tfidf_file1000-10.xlsx')\n",
        "\n",
        "# Separate features (X) and target labels (y)\n",
        "X = df.drop(columns=['text', 'target'], errors='ignore')\n",
        "y = df['target']\n",
        "\n",
        "# Split the data into training and testing sets (70-30 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Multinomial Naive Bayes classifier\n",
        "nb_model = MultinomialNB()\n",
        "\n",
        "# Train the model\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = nb_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "# Set digits=3 to format the classification report to three decimal points\n",
        "classification_rep = classification_report(y_test, y_pred, digits=3)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "ram_usage = psutil.virtual_memory().used / (1024 ** 3)  # RAM usage in GB\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"--- Multinomial Naive Bayes Model ---\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"\\nClassification Report:\\n\", classification_rep)\n",
        "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
        "print(\"\\nSuccessfully detected spam (True Positives):\", conf_matrix[1, 1])\n",
        "print(f\"\\nRAM Usage: {ram_usage:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. K-Nearest-Neighbor"
      ],
      "metadata": {
        "id": "fBmvIwy9U4hu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wL8UYEGU3aXI",
        "outputId": "e80eae1f-fbd5-4feb-e819-f7dcaa3e3d17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed time: 186.17 seconds\n",
            "--- K-Nearest Neighbors Model ---\n",
            "Accuracy: 0.7713333333333333\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0      0.936     0.570     0.709      1463\n",
            "           1      0.702     0.963     0.812      1537\n",
            "\n",
            "    accuracy                          0.771      3000\n",
            "   macro avg      0.819     0.766     0.760      3000\n",
            "weighted avg      0.816     0.771     0.761      3000\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 834  629]\n",
            " [  57 1480]]\n",
            "\n",
            "Successfully detected spam (True Positives): 1480\n",
            "\n",
            "RAM Usage: 1.83 GB\n"
          ]
        }
      ],
      "source": [
        "#K-Nearest-Neighbor (KNN)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import psutil  # Import the psutil library for memory usage\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "# Load feature extracted dataset - TF-IDF\n",
        "df = pd.read_excel('/content/tfidf_file1000-10.xlsx')\n",
        "\n",
        "# Separate features (X) and target labels (y)\n",
        "X = df.drop(columns=['text', 'target'], errors='ignore')\n",
        "y = df['target']\n",
        "\n",
        "# Split the data into training and testing sets (70-30 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the K-Nearest Neighbors classifier\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Train the model\n",
        "knn_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = knn_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred, digits=3)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "ram_usage = psutil.virtual_memory().used / (1024 ** 3)  # RAM usage in GB\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"--- K-Nearest Neighbors Model ---\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"\\nClassification Report:\\n\", classification_rep)\n",
        "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
        "print(\"\\nSuccessfully detected spam (True Positives):\", conf_matrix[1, 1])\n",
        "print(f\"\\nRAM Usage: {ram_usage:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Logsitic Regression (Meta-model)"
      ],
      "metadata": {
        "id": "tQMpapLUU67F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4Ncifo_XAPE",
        "outputId": "049092bf-10ae-47c9-dca6-8bbce3adb0a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed time: 292.79 seconds\n",
            "--- Logistic Regression Model ---\n",
            "Accuracy: 94.56%\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0      0.947     0.944     0.945      2388\n",
            "           1      0.945     0.947     0.946      2412\n",
            "\n",
            "    accuracy                          0.946      4800\n",
            "   macro avg      0.946     0.946     0.946      4800\n",
            "weighted avg      0.946     0.946     0.946      4800\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[2254  134]\n",
            " [ 127 2285]]\n",
            "\n",
            "Successfully detected spam (True Positives): 2285\n",
            "\n",
            "RAM Usage: 2.31 GB\n"
          ]
        }
      ],
      "source": [
        "#LOGISTIC REGRESSION\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import psutil  # Import the psutil library for memory usage\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Load feature extracted dataset - TF-IDF\n",
        "df = pd.read_excel('/content/tfidf_file1000-16.xlsx')\n",
        "\n",
        "# Separate features (X) and target labels (y)\n",
        "X = df.drop(columns=['text', 'target'], errors='ignore')\n",
        "y = df['target']\n",
        "\n",
        "# Split the data into training and testing sets (70-30 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "log_reg_model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "\n",
        "# Train the model\n",
        "log_reg_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = log_reg_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred, digits=3)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "ram_usage = psutil.virtual_memory().used / (1024 ** 3)  # RAM usage in GB\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
        "\n",
        "# Output results\n",
        "print(\"--- Logistic Regression Model ---\")\n",
        "print(f\"Accuracy: {accuracy:.2%}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_rep)\n",
        "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
        "print(\"\\nSuccessfully detected spam (True Positives):\", conf_matrix[1, 1])\n",
        "print(f\"\\nRAM Usage: {ram_usage:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Stacking (MNB, SVM, DT) + (LR) - Combination 1"
      ],
      "metadata": {
        "id": "Maxq8aIbU-f_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVXAWC0C1k0C",
        "outputId": "fa29ee20-7240-470e-ef6a-6e0eafd70ab0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed time: 78.08 seconds\n",
            "RAM Usage: 2.06 GB\n",
            "\n",
            "--- Optimized Stacking Model ---\n",
            "Accuracy: 98.58%\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0      0.984     0.986     0.985       578\n",
            "           1      0.987     0.986     0.986       622\n",
            "\n",
            "    accuracy                          0.986      1200\n",
            "   macro avg      0.986     0.986     0.986      1200\n",
            "weighted avg      0.986     0.986     0.986      1200\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[570   8]\n",
            " [  9 613]]\n",
            "\n",
            "Successfully detected spam (True Positives): 613\n"
          ]
        }
      ],
      "source": [
        "#STACKING WITH MNB SVM DT\n",
        "\n",
        "import pandas as pd\n",
        "import time\n",
        "import psutil\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Load feature extracted dataset - TF-IDF\n",
        "df = pd.read_excel('/content/tfidf_file1000-4.xlsx', usecols=lambda column: column not in ['text'])\n",
        "\n",
        "# Prepare features and labels\n",
        "X = df.drop(columns=['target'], errors='ignore')\n",
        "y = df['target']\n",
        "\n",
        "# Feature selection\n",
        "k_best_features = SelectKBest(chi2, k=500).fit_transform(X, y)\n",
        "\n",
        "# Train-Test split (70-30)\n",
        "X_train, X_test, y_train, y_test = train_test_split(k_best_features, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define base models with hyperparameter optimization\n",
        "base_models = [\n",
        "    ('mnb', MultinomialNB(alpha=0.01)),\n",
        "    ('svc', SVC(kernel='linear', C=1.0, probability=True)),\n",
        "    ('dt', DecisionTreeClassifier(max_depth=10, random_state=42))\n",
        "]\n",
        "\n",
        "# Define optimized meta-model (Logistic Regression with GridSearch)\n",
        "param_grid = {\n",
        "    'solver': ['liblinear', 'lbfgs'],\n",
        "    'C': [0.1, 1.0, 10]\n",
        "}\n",
        "\n",
        "meta_model = GridSearchCV(LogisticRegression(solver='liblinear'), param_grid, cv=5)\n",
        "\n",
        "# Initialize StackingClassifier with optimized models\n",
        "stacking_model = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=5, passthrough=True, n_jobs=-1)\n",
        "\n",
        "# Train the model\n",
        "stacking_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = stacking_model.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred, digits=3)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "successfully_detected_spam = conf_matrix[1, 1]  # True Positives (Spam detected correctly)\n",
        "\n",
        "# Measure RAM Usage\n",
        "ram_usage = psutil.virtual_memory().used / (1024 ** 3)\n",
        "\n",
        "# Measure elapsed time\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "# Print results\n",
        "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
        "print(f\"RAM Usage: {ram_usage:.2f} GB\")\n",
        "\n",
        "print(\"\\n--- Optimized Stacking Model ---\")\n",
        "print(f\"Accuracy: {accuracy:.2%}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_rep)\n",
        "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
        "print(f\"\\nSuccessfully detected spam (True Positives): {successfully_detected_spam}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Stacking (MNB, KNN, DT) + (LR) - Combination 2"
      ],
      "metadata": {
        "id": "hPhlIqREVCwF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e17-nzBdg86K",
        "outputId": "601032ca-c0f1-4c02-84f3-cc4e739d8754"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed time: 78.36 seconds\n",
            "RAM Usage: 1.84 GB\n",
            "\n",
            "--- Optimized Stacking Model ---\n",
            "Accuracy: 98.92%\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0      0.986     0.991     0.989       578\n",
            "           1      0.992     0.987     0.990       622\n",
            "\n",
            "    accuracy                          0.989      1200\n",
            "   macro avg      0.989     0.989     0.989      1200\n",
            "weighted avg      0.989     0.989     0.989      1200\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[573   5]\n",
            " [  8 614]]\n",
            "\n",
            "Successfully detected spam (True Positives): 614\n"
          ]
        }
      ],
      "source": [
        "#STACKING WITH MNB KNN DT\n",
        "\n",
        "import pandas as pd\n",
        "import time\n",
        "import psutil\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Load feature extracted dataset - TF-IDF\n",
        "df = pd.read_excel('/content/tfidf_file1000-4.xlsx', usecols=lambda column: column not in ['text'])\n",
        "\n",
        "# Prepare features and labels\n",
        "X = df.drop(columns=['target'], errors='ignore')\n",
        "y = df['target']\n",
        "\n",
        "# Feature selection\n",
        "k_best_features = SelectKBest(chi2, k=500).fit_transform(X, y)\n",
        "\n",
        "# Train-Test split (70-30)\n",
        "X_train, X_test, y_train, y_test = train_test_split(k_best_features, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define base models with hyperparameter optimization\n",
        "base_models = [\n",
        "    ('mnb', MultinomialNB(alpha=0.01)),\n",
        "    ('knn', KNeighborsClassifier(n_neighbors=5)),\n",
        "    ('dt', DecisionTreeClassifier(max_depth=10, random_state=42))\n",
        "]\n",
        "\n",
        "meta_model = LogisticRegression(solver='liblinear')\n",
        "\n",
        "# Initialize StackingClassifier with optimized models\n",
        "stacking_model = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=5, passthrough=True, n_jobs=-1)\n",
        "\n",
        "# Train the model\n",
        "stacking_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = stacking_model.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred, digits=3)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "successfully_detected_spam = conf_matrix[1, 1]\n",
        "\n",
        "# Measure RAM Usage\n",
        "ram_usage = psutil.virtual_memory().used / (1024 ** 3)\n",
        "\n",
        "# Measure elapsed time\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "# Print results\n",
        "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
        "print(f\"RAM Usage: {ram_usage:.2f} GB\")\n",
        "\n",
        "print(\"\\n--- Optimized Stacking Model ---\")\n",
        "print(f\"Accuracy: {accuracy:.2%}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_rep)\n",
        "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
        "print(f\"\\nSuccessfully detected spam (True Positives): {successfully_detected_spam}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Stacking (MNB, Perceptron, DT) + (LR) - Combination 3"
      ],
      "metadata": {
        "id": "R6RrRgdnVHLa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWVsLLIbi8-j",
        "outputId": "ddb137d7-6af0-445f-be2e-5dac92ee29d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed time: 64.06 seconds\n",
            "RAM Usage: 1.91 GB\n",
            "\n",
            "--- Optimized Stacking Model ---\n",
            "Accuracy: 98.08%\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0      0.968     0.993     0.980       578\n",
            "           1      0.993     0.969     0.981       622\n",
            "\n",
            "    accuracy                          0.981      1200\n",
            "   macro avg      0.981     0.981     0.981      1200\n",
            "weighted avg      0.981     0.981     0.981      1200\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[574   4]\n",
            " [ 19 603]]\n",
            "\n",
            "Successfully detected spam (True Positives): 603\n"
          ]
        }
      ],
      "source": [
        "#STACKING WITH MNB Perceptron DT\n",
        "\n",
        "import pandas as pd\n",
        "import time\n",
        "import psutil\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Load feature extracted dataset - TF-IDF\n",
        "df = pd.read_excel('/content/tfidf_file1000-4.xlsx', usecols=lambda column: column not in ['text'])\n",
        "\n",
        "# Prepare features and labels\n",
        "X = df.drop(columns=['target'], errors='ignore')\n",
        "y = df['target']\n",
        "\n",
        "# Feature selection\n",
        "k_best_features = SelectKBest(chi2, k=500).fit_transform(X, y)\n",
        "\n",
        "# Train-Test split (70-30)\n",
        "X_train, X_test, y_train, y_test = train_test_split(k_best_features, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define base models with hyperparameter optimization\n",
        "base_models = [\n",
        "    ('mnb', MultinomialNB(alpha=0.01)),\n",
        "    ('perceptron', Perceptron(max_iter=1000, tol=1e-3)),\n",
        "    ('dt', DecisionTreeClassifier(max_depth=10, random_state=42))\n",
        "]\n",
        "\n",
        "# Define optimized meta-model (Logistic Regression with GridSearch)\n",
        "param_grid = {\n",
        "    'solver': ['liblinear', 'lbfgs'],\n",
        "    'C': [0.1, 1.0, 10]\n",
        "}\n",
        "\n",
        "meta_model = GridSearchCV(LogisticRegression(solver='liblinear'), param_grid, cv=5)\n",
        "\n",
        "# Initialize StackingClassifier with optimized models\n",
        "stacking_model = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=5, passthrough=True, n_jobs=-1)\n",
        "\n",
        "# Train the model\n",
        "stacking_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = stacking_model.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred, digits=3)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "successfully_detected_spam = conf_matrix[1, 1]\n",
        "\n",
        "# Measure RAM Usage\n",
        "ram_usage = psutil.virtual_memory().used / (1024 ** 3)\n",
        "\n",
        "# Measure elapsed time\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "# Print results\n",
        "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
        "print(f\"RAM Usage: {ram_usage:.2f} GB\")\n",
        "\n",
        "print(\"\\n--- Optimized Stacking Model ---\")\n",
        "print(f\"Accuracy: {accuracy:.2%}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_rep)\n",
        "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
        "print(f\"\\nSuccessfully detected spam (True Positives): {successfully_detected_spam}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. Stacking (MNB, LightGBM, RF) + (LR) - Combination 4"
      ],
      "metadata": {
        "id": "ymHj0YDHVMlw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aery_yOnk8zy",
        "outputId": "341daa1d-79cc-466e-cd4c-086f65b4b339"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 1378, number of negative: 1422\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029549 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 49560\n",
            "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 990\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492143 -> initscore=-0.031431\n",
            "[LightGBM] [Info] Start training from score -0.031431\n",
            "[LightGBM] [Info] Number of positive: 918, number of negative: 948\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025374 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 35252\n",
            "[LightGBM] [Info] Number of data points in the train set: 1866, number of used features: 978\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.491961 -> initscore=-0.032157\n",
            "[LightGBM] [Info] Start training from score -0.032157\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 919, number of negative: 948\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016979 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 34244\n",
            "[LightGBM] [Info] Number of data points in the train set: 1867, number of used features: 974\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492234 -> initscore=-0.031068\n",
            "[LightGBM] [Info] Start training from score -0.031068\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 919, number of negative: 948\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016796 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 34507\n",
            "[LightGBM] [Info] Number of data points in the train set: 1867, number of used features: 972\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492234 -> initscore=-0.031068\n",
            "[LightGBM] [Info] Start training from score -0.031068\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Elapsed time: 72.72 seconds\n",
            "RAM Usage: 1.91 GB\n",
            "\n",
            "--- Simplified Stacking Model ---\n",
            "Accuracy: 99.00%\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0      0.995     0.984     0.990       578\n",
            "           1      0.986     0.995     0.990       622\n",
            "\n",
            "    accuracy                          0.990      1200\n",
            "   macro avg      0.990     0.990     0.990      1200\n",
            "weighted avg      0.990     0.990     0.990      1200\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[569   9]\n",
            " [  3 619]]\n",
            "\n",
            "Successfully detected spam (True Positives): 619\n"
          ]
        }
      ],
      "source": [
        "#STACKING WITH MNB LightGBM RF\n",
        "\n",
        "import pandas as pd\n",
        "import time\n",
        "import psutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Load feature extracted dataset - TF-IDF\n",
        "df = pd.read_excel('/content/tfidf_file1000-4.xlsx', usecols=lambda column: column not in ['text'])\n",
        "\n",
        "# Prepare features and labels\n",
        "X = df.drop(columns=['target'], errors='ignore')\n",
        "y = df['target']\n",
        "\n",
        "# Train-Test split (70-30)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define base models\n",
        "base_models = [\n",
        "    ('mnb', MultinomialNB(alpha=0.01)),\n",
        "    ('lightgbm', lgb.LGBMClassifier()),\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "]\n",
        "\n",
        "# Define simple meta-model\n",
        "meta_model = LogisticRegression(solver='liblinear')\n",
        "\n",
        "# Initialize StackingClassifier without passthrough\n",
        "stacking_model = StackingClassifier(base_models, meta_model, cv=3)\n",
        "\n",
        "# Train the model\n",
        "stacking_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = stacking_model.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred, digits=3)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "successfully_detected_spam = conf_matrix[1, 1]\n",
        "\n",
        "# Measure RAM Usage\n",
        "ram_usage = psutil.virtual_memory().used / (1024 ** 3)\n",
        "\n",
        "# Measure elapsed time\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "# Print results\n",
        "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
        "print(f\"RAM Usage: {ram_usage:.2f} GB\")\n",
        "\n",
        "print(\"\\n--- Simplified Stacking Model ---\")\n",
        "print(f\"Accuracy: {accuracy:.2%}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_rep)\n",
        "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
        "print(f\"\\nSuccessfully detected spam (True Positives): {successfully_detected_spam}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. Final Stacking Technique (MNB, KNN, RF) + (LR) - Combination 5"
      ],
      "metadata": {
        "id": "9htV6SRtlvpo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6yy2DtXm_WL",
        "outputId": "7da39ea7-582f-413c-ef6e-a29e0d2cb77c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed time: 66.31 seconds\n",
            "RAM Usage: 2.03 GB\n",
            "\n",
            "--- Simplified Stacking Model ---\n",
            "Accuracy: 99.08%\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0      0.990     0.991     0.990       578\n",
            "           1      0.992     0.990     0.991       622\n",
            "\n",
            "    accuracy                          0.991      1200\n",
            "   macro avg      0.991     0.991     0.991      1200\n",
            "weighted avg      0.991     0.991     0.991      1200\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[573   5]\n",
            " [  6 616]]\n",
            "\n",
            "Successfully detected spam (True Positives): 616\n"
          ]
        }
      ],
      "source": [
        "#STACKING TECHNIQUE\n",
        "\n",
        "import pandas as pd\n",
        "import time\n",
        "import psutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Load feature extracted dataset - TF-IDF\n",
        "df = pd.read_excel('/content/tfidf_file1000-4.xlsx', usecols=lambda column: column not in ['text'])\n",
        "\n",
        "# Prepare features and labels\n",
        "X = df.drop(columns=['target'], errors='ignore')\n",
        "y = df['target']\n",
        "\n",
        "# Train-Test split (70-30)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define base models\n",
        "base_models = [\n",
        "    ('mnb', MultinomialNB(alpha=0.01)),\n",
        "    ('knn', KNeighborsClassifier(n_neighbors=5)),\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "]\n",
        "\n",
        "# Define meta-model\n",
        "meta_model = LogisticRegression(solver='liblinear', random_state=42)\n",
        "\n",
        "# Initialize StackingClassifier\n",
        "stacking_model = StackingClassifier(base_models, meta_model, cv=3, passthrough=True)\n",
        "\n",
        "# Train the model\n",
        "stacking_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = stacking_model.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred, digits=3)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "successfully_detected_spam = conf_matrix[1, 1]  # True Positives (Spam detected correctly)\n",
        "\n",
        "# Measure RAM Usage\n",
        "ram_usage = psutil.virtual_memory().used / (1024 ** 3)\n",
        "\n",
        "# Measure elapsed time\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "# Print results\n",
        "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
        "print(f\"RAM Usage: {ram_usage:.2f} GB\")\n",
        "print(\"\\n--- Simplified Stacking Model ---\")\n",
        "print(f\"Accuracy: {accuracy:.2%}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_rep)\n",
        "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
        "print(f\"\\nSuccessfully detected spam (True Positives): {successfully_detected_spam}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}